# Action Chunk Based PPO Training for Pi0FAST Actor-Critic

è¿™æ˜¯ä¸€ä¸ªåŸºäºaction chunkçš„PPOè®­ç»ƒå®ç°ï¼Œä¸“é—¨ä¸ºPi0FAST Actor-Criticæ¨¡å‹å’ŒAlohaçœŸå®ç¯å¢ƒè®¾è®¡ã€‚

## æ ¸å¿ƒç‰¹æ€§

### ğŸ”„ **Action Chunkçº§åˆ«çš„PPO**
- **æ•°æ®ç²’åº¦**: æ¯ä¸ªaction chunkå¯¹åº”ä¸€ä¸ªreward
- **GAEè®¡ç®—**: åœ¨chunkçº§åˆ«è®¡ç®—ä¼˜åŠ¿ä¼°è®¡
- **ç»å…¸PPOæŸå¤±**: ä½¿ç”¨æ ‡å‡†çš„actor losså…¬å¼

### ğŸ§  **æ¨¡å‹æ¶æ„å¢å¼º**
- **æ‰©å±•çš„ActorCriticç±»**: æ·»åŠ äº†chunkçº§åˆ«çš„æ–¹æ³•
- **Tokençº§åˆ«æ¦‚ç‡**: é€šè¿‡logitsç›´æ¥è®¡ç®—action chunkæ¦‚ç‡
- **Valueå‡½æ•°**: æ”¯æŒobs-action chunkå¯¹çš„ä»·å€¼ä¼°è®¡

### ğŸ¦¾ **Alohaç¯å¢ƒé›†æˆ**
- **çœŸå®æœºå™¨äºº**: æ”¯æŒAlohaåŒè‡‚æœºå™¨äºº
- **Chunkæ‰§è¡Œ**: å®Œæ•´æ‰§è¡Œaction sequenceå¹¶æ”¶é›†ç´¯ç§¯reward
- **ç¯å¢ƒåŒ…è£…**: é€‚é…PPOè®­ç»ƒæ¥å£

## æ–‡ä»¶ç»“æ„

```
scripts/
â”œâ”€â”€ train_ppo_chunk_based.py        # ä¸»è®­ç»ƒè„šæœ¬
â”œâ”€â”€ README_PPO_CHUNK.md            # æœ¬è¯´æ˜æ–‡æ¡£
â””â”€â”€ action_chunk_ppo_example.py     # ç†è®ºç¤ºä¾‹ä»£ç 

src/openpi/models/
â””â”€â”€ pi0_fast_actor_critic.py       # æ‰©å±•çš„ActorCriticç±»
```

## ğŸ¯ é¢„è®­ç»ƒæ¨¡å‹åŠ è½½

è¯¥è®­ç»ƒè„šæœ¬æ”¯æŒä»æœ¬åœ°é¢„è®­ç»ƒPi0FASTæ¨¡å‹è¿›è¡Œonline fine-tuningï¼š

```python
# é…ç½®æœ¬åœ°é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„
debug_config = PPOConfig(
    pretrained_checkpoint_path="./checkpoints/pi0_fast_base/params",  # æœ¬åœ°è·¯å¾„
    model_config=Pi0FASTActorCriticConfig(
        action_dim=7,           # âœ… åŒ¹é…é¢„è®­ç»ƒæ¨¡å‹
        action_horizon=10,      # âœ… åŒ¹é…é¢„è®­ç»ƒæ¨¡å‹
        max_token_len=180,      # âœ… åŒ¹é…é¢„è®­ç»ƒæ¨¡å‹
    ),
)
```

**ç»Ÿä¸€æ ¼å¼æ”¯æŒ**ï¼š
- ğŸ”„ **è‡ªåŠ¨æ£€æµ‹**: æ™ºèƒ½è¯†åˆ«Pi0FAST ILæ ¼å¼ vs RLæ ¼å¼
- âœ… **ILâ†’RL**: Pi0FASTé¢„è®­ç»ƒå‚æ•° â†’ `actor_backbone` + éšæœº`value_head`
- âœ… **RLâ†’RL**: å®Œæ•´Actor-Criticå‚æ•°ç›´æ¥åŠ è½½
- âœ… **å½¢çŠ¶éªŒè¯**: è‡ªåŠ¨æ£€æŸ¥å‚æ•°å…¼å®¹æ€§
- âœ… **ä¼˜é›…é™çº§**: åŠ è½½å¤±è´¥æ—¶è‡ªåŠ¨å›é€€åˆ°éšæœºåˆå§‹åŒ–

**æ”¯æŒçš„åŠ è½½åœºæ™¯**ï¼š
```python
# 1. ä»Pi0FAST ILé¢„è®­ç»ƒå¼€å§‹ (7DOF â†’ actor_backbone)
pretrained_checkpoint_path = "./checkpoints/pi0_fast_base/params"

# 2. ä»ä¹‹å‰çš„RLè®­ç»ƒç»§ç»­ (å®Œæ•´Actor-Critic)  
pretrained_checkpoint_path = "./checkpoints/ppo_chunk_aloha/step_100/params"

# 3. åœ¨ä¸åŒRLå®éªŒé—´åˆ‡æ¢
pretrained_checkpoint_path = "./checkpoints/experiment_A/step_500/params"
```

**ä¿å­˜æ ¼å¼**ï¼š
- ğŸ’¾ **ç»Ÿä¸€æ ¼å¼**: æ‰€æœ‰ä¿å­˜çš„checkpointéƒ½ä½¿ç”¨RLæ ¼å¼ (`actor_backbone` + `value_head`)
- ğŸ“‚ **ç›®å½•ç»“æ„**: `./checkpoints/{config.checkpoint_dir}/step_{N}/params/`
- ğŸ“„ **å…ƒæ•°æ®**: åŒ…å«è®­ç»ƒçŠ¶æ€å’Œæ¨¡å‹é…ç½®ä¿¡æ¯ (`metadata.json`)
- ğŸ”„ **å…¼å®¹æ€§**: ä¿å­˜çš„checkpointå¯è¢«åç»­è®­ç»ƒç›´æ¥åŠ è½½

## æ ¸å¿ƒç®—æ³•

### 1. Action Chunkæ¦‚ç‡è®¡ç®—

```python
def compute_action_chunk_log_prob(self, observation, actions, rng):
    # 1. å°†action chunkè½¬æ¢ä¸ºtokenåºåˆ—
    tokens = tokenizer.tokenize(actions)  # [t1, t2, ..., tn]
    
    # 2. è·å¾—æ¨¡å‹å¯¹æ¯ä¸ªtokençš„é¢„æµ‹logits
    logits_sequence = model.get_logits(observation)  # [steps, vocab_size]
    
    # 3. è®¡ç®—tokenåºåˆ—çš„æ€»logæ¦‚ç‡ï¼ˆç»å…¸æ–¹æ³•ï¼‰
    log_probs = log_softmax(logits_sequence)
    token_log_probs = log_probs[range(n), tokens]
    chunk_log_prob = sum(token_log_probs)  # æ‰€æœ‰tokenæ¦‚ç‡ä¹‹å’Œ
    
    return chunk_log_prob
```

### 2. ç»å…¸PPO Actor Loss

```python
def compute_ppo_loss(obs, actions, old_log_prob, advantage):
    # 1. è®¡ç®—æ–°æ¨¡å‹çš„action chunkæ¦‚ç‡
    new_log_prob = model.compute_action_chunk_log_prob(obs, actions)
    
    # 2. è®¡ç®—æ¦‚ç‡æ¯”å€¼
    ratio = exp(new_log_prob - old_log_prob)
    
    # 3. PPO clippedæŸå¤±ï¼ˆç»å…¸å…¬å¼ï¼‰
    surr1 = ratio * advantage
    surr2 = clip(ratio, 1-Îµ, 1+Îµ) * advantage
    actor_loss = -min(surr1, surr2)
    
    return actor_loss
```

### 3. Chunkçº§åˆ«GAE

```python
def compute_chunk_gae(chunk_rewards, chunk_values, gamma, lambda):
    advantages = zeros_like(chunk_rewards)
    
    gae = 0
    for t in reversed(range(len(chunk_rewards))):
        delta = chunk_rewards[t] + gamma * next_value - chunk_values[t]
        gae = delta + gamma * lambda * gae
        advantages[t] = gae
    
    return advantages
```

## ä½¿ç”¨æ–¹æ³•

### å¿«é€Ÿå¼€å§‹

```bash
# è°ƒè¯•æ¨¡å¼ï¼ˆå¿«é€Ÿæµ‹è¯•ï¼‰
python scripts/train_ppo_chunk_based.py --config=debug --exp_name=test_chunk_ppo

# å®Œæ•´è®­ç»ƒ
python scripts/train_ppo_chunk_based.py --config=full --exp_name=aloha_chunk_ppo_production

# ç¦ç”¨WandBæ—¥å¿—
python scripts/train_ppo_chunk_based.py --config=debug --exp_name=test_chunk_ppo --no_wandb
```

### é…ç½®é€‰é¡¹

#### Debugé…ç½®ï¼ˆå¿«é€Ÿæµ‹è¯•ï¼‰
```python
debug_config = PPOConfig(
    total_timesteps=5_000,          # 5Kæ­¥
    num_chunks_per_update=32,       # æ¯æ¬¡æ›´æ–°32ä¸ªchunks
    num_epochs_per_update=2,        # 2ä¸ªPPO epochs
    batch_size=8,                   # å°æ‰¹æ¬¡
    learning_rate=1e-4,            # è¾ƒä½å­¦ä¹ ç‡
)
```

#### Fullé…ç½®ï¼ˆç”Ÿäº§è®­ç»ƒï¼‰
```python
full_config = PPOConfig(
    total_timesteps=500_000,        # 500Kæ­¥
    num_chunks_per_update=128,      # æ¯æ¬¡æ›´æ–°128ä¸ªchunks
    num_epochs_per_update=4,        # 4ä¸ªPPO epochs
    batch_size=32,                  # æ ‡å‡†æ‰¹æ¬¡
    learning_rate=3e-4,            # æ ‡å‡†å­¦ä¹ ç‡
)
```

### æ¨¡å‹é…ç½®

```python
model_config = Pi0FASTActorCriticConfig(
    action_dim=14,              # Aloha 14è‡ªç”±åº¦
    action_horizon=16,          # Action chunkå¤§å°
    max_token_len=180,          # æœ€å¤§tokené•¿åº¦
    paligemma_variant="gemma_2b",  # 2Bæ¨¡å‹
    value_head_hidden_dim=512,     # Valueå¤´ç»´åº¦
    max_value_steps=32,         # Valueè®¡ç®—æœ€å¤§æ­¥æ•°
)
```

## è®­ç»ƒæµç¨‹

### 1. æ•°æ®æ”¶é›†é˜¶æ®µ
```python
for chunk_idx in range(num_chunks_per_update):
    # é‡‡æ ·action chunk
    actions = model.sample_actions(observation)
    
    # è®¡ç®—chunkä»·å€¼å’Œæ¦‚ç‡
    chunk_value = model.compute_action_chunk_value(obs, actions)
    chunk_log_prob = model.compute_action_chunk_log_prob(obs, actions)
    
    # æ‰§è¡Œå®Œæ•´chunkï¼Œæ”¶é›†ç´¯ç§¯reward
    next_obs, chunk_reward, done, info = env.step_chunk(actions)
    
    # å­˜å‚¨chunkçº§åˆ«æ•°æ®
    buffer.add(obs, actions, chunk_reward, chunk_value, chunk_log_prob)
```

### 2. GAEè®¡ç®—é˜¶æ®µ
```python
# åœ¨chunkçº§åˆ«è®¡ç®—GAE
rewards = [chunk_rewards...]
values = [chunk_values...]

for t in reversed(range(len(rewards))):
    delta = rewards[t] + gamma * next_value - values[t]
    gae = delta + gamma * lambda * gae
    advantages[t] = gae
```

### 3. æ¨¡å‹æ›´æ–°é˜¶æ®µ
```python
for epoch in range(num_epochs_per_update):
    for batch in create_batches(buffer):
        # è®¡ç®—æ–°çš„chunkæ¦‚ç‡
        new_log_probs = [model.compute_action_chunk_log_prob(obs, actions) 
                        for obs, actions in batch]
        
        # PPOæŸå¤±
        ratios = exp(new_log_probs - old_log_probs)
        surr1 = ratios * advantages
        surr2 = clip(ratios) * advantages
        actor_loss = -min(surr1, surr2)
        
        # æ›´æ–°æ¨¡å‹
        update_model(actor_loss + value_loss)
```

## å…³é”®ä¼˜åŠ¿

### âœ… **ç†è®ºæ­£ç¡®æ€§**
- ç¬¦åˆPi0FASTçš„tokenåŒ–è®¾è®¡
- ä½¿ç”¨ç»å…¸PPOå…¬å¼
- åœ¨åˆé€‚çš„æŠ½è±¡å±‚çº§æ“ä½œ

### âœ… **è®¡ç®—æ•ˆç‡**
- Chunkçº§åˆ«çš„GAEå‡å°‘è®¡ç®—é‡
- æ‰¹é‡å¤„ç†tokenåºåˆ—
- æœ‰æ•ˆåˆ©ç”¨GPUå¹¶è¡ŒåŒ–

### âœ… **å®ç°ç®€æ´**
- æ ‡å‡†PPOç®—æ³•ï¼Œæ— å¤æ‚ä¿®æ”¹
- æ¸…æ™°çš„æ•°æ®æµ
- æ˜“äºè°ƒè¯•å’Œæ‰©å±•

### âœ… **å®é™…å¯ç”¨**
- æ”¯æŒçœŸå®Alohaæœºå™¨äºº
- å®Œæ•´çš„æ—¥å¿—å’Œæ£€æŸ¥ç‚¹ç³»ç»Ÿ
- çµæ´»çš„é…ç½®é€‰é¡¹

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### 1. å‡†å¤‡é¢„è®­ç»ƒæ¨¡å‹

ç¡®ä¿é¢„è®­ç»ƒPi0FASTæ¨¡å‹æ”¾ç½®åœ¨æ­£ç¡®çš„æœ¬åœ°è·¯å¾„ï¼š

```bash
# é¢„è®­ç»ƒæ¨¡å‹ç›®å½•ç»“æ„
./checkpoints/
â””â”€â”€ pi0_fast_base/
    â””â”€â”€ params/          # Pi0FASTé¢„è®­ç»ƒå‚æ•°
        â”œâ”€â”€ checkpoint
        â””â”€â”€ ... (å…¶ä»–checkpointæ–‡ä»¶)
```

### 2. è¿è¡Œè®­ç»ƒ

```bash
# ä½¿ç”¨debugé…ç½®ï¼ˆå¿«é€Ÿæµ‹è¯•ï¼ŒåŒ…å«é¢„è®­ç»ƒæ¨¡å‹ï¼‰
python scripts/train_ppo_chunk_based.py --config=debug --exp_name=ppo_test

# ä½¿ç”¨å®Œæ•´é…ç½®ï¼ˆé•¿æ—¶é—´è®­ç»ƒï¼‰  
python scripts/train_ppo_chunk_based.py --config=full --exp_name=ppo_full_run
```

### 3. è‡ªå®šä¹‰é¢„è®­ç»ƒè·¯å¾„

```python
# ä¿®æ”¹é…ç½®ä¸­çš„é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„
pretrained_checkpoint_path = "/path/to/your/pi0fast/params"
```

### 4. æ— é¢„è®­ç»ƒæ¨¡å¼

```python
# è®¾ç½®ä¸ºNoneä»¥ä»å¤´å¼€å§‹è®­ç»ƒ
pretrained_checkpoint_path = None
```

## ç›‘æ§æŒ‡æ ‡

è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šè®°å½•ä»¥ä¸‹æŒ‡æ ‡ï¼š

- **Episode Reward**: æ¯ä¸ªepisodeçš„æ€»å¥–åŠ±
- **Chunk Reward**: æ¯ä¸ªaction chunkçš„å¥–åŠ±
- **Policy Loss**: PPO actoræŸå¤±
- **Value Loss**: ä»·å€¼å‡½æ•°æŸå¤±
- **Training Progress**: æ›´æ–°è¿›åº¦å’Œæ€§èƒ½

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **å†…å­˜ä¸è¶³**
   - å‡å°‘`num_chunks_per_update`
   - é™ä½`batch_size`
   - ä½¿ç”¨CPUè®­ç»ƒ

2. **æ”¶æ•›ç¼“æ…¢**
   - è°ƒæ•´å­¦ä¹ ç‡
   - ä¿®æ”¹PPOå‚æ•°ï¼ˆ`clip_coef`, `ent_coef`ï¼‰
   - æ£€æŸ¥rewardå‡½æ•°è®¾è®¡

3. **ç¯å¢ƒè¿æ¥é—®é¢˜**
   - ç¡®è®¤Alohaæœºå™¨äººç¡¬ä»¶è¿æ¥
   - æ£€æŸ¥ç¯å¢ƒé…ç½®
   - éªŒè¯æƒé™è®¾ç½®

### è°ƒè¯•æŠ€å·§

```bash
# ä½¿ç”¨debugé…ç½®è¿›è¡Œå¿«é€Ÿæµ‹è¯•
python scripts/train_ppo_chunk_based.py --config=debug --no_wandb

# æ£€æŸ¥chunk rewardåˆ†å¸ƒ
# åœ¨ä»£ç ä¸­æ·»åŠ : print(f"Chunk reward: {chunk_reward}")

# ç›‘æ§æ¦‚ç‡æ¯”å€¼
# åœ¨ä»£ç ä¸­æ·»åŠ : print(f"Ratio: {ratio}")
```

## æ‰©å±•æ–¹å‘

### å¯èƒ½çš„æ”¹è¿›

1. **æ›´å¤æ‚çš„Valueå‡½æ•°**
   - çœŸæ­£çš„V(s,a)è®¡ç®—
   - åŸºäºattentionçš„obs-actionèåˆ

2. **è‡ªé€‚åº”chunkå¤§å°**
   - åŠ¨æ€è°ƒæ•´action_horizon
   - åŸºäºä»»åŠ¡å¤æ‚åº¦çš„é€‚åº”

3. **å¤šæ¨¡æ€reward**
   - ç¯å¢ƒreward + äººç±»åé¦ˆ
   - å†…åœ¨åŠ¨æœºå¥–åŠ±

4. **åˆ†å¸ƒå¼è®­ç»ƒ**
   - å¤šGPUå¹¶è¡Œ
   - å¼‚æ­¥Actor-Learneræ¶æ„

## å‚è€ƒ

- [PPOåŸå§‹è®ºæ–‡](https://arxiv.org/abs/1707.06347)
- [Pi0FASTè®ºæ–‡](ç›¸å…³é“¾æ¥)
- [OpenPIæ¡†æ¶æ–‡æ¡£](../README.md)
- [Alohaæœºå™¨äººæ–‡æ¡£](../examples/aloha_real/README.md)
